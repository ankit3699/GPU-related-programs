{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ankit_matrix_mul.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POT131dQaUV3"
      },
      "source": [
        "#Matrix Multiplication using numba \n",
        "We know that Matrix operations are very time consuming and Matrix multiplication is one of the prolonged process. To overcome this we try to use matrix multiplication. Here we’ll compare time required to perform Matrix multiplication with CPU and GPU. Also, we’ll try to find time required to compute matrix multiplication of two 10000x10000 matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn8s0pXHYsw6"
      },
      "source": [
        "import time\n",
        "import numba\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from numba import cuda,jit"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX2pCrtUFsdn"
      },
      "source": [
        "wA = 10000\n",
        "hA = 10000\n",
        "wB = hA\n",
        "hB = wA"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpRtEPw2T_zt"
      },
      "source": [
        "Firstly, we need to know some basics like what is a kernel, threads and blocks. \\\\\n",
        "A **CUDA Kernel** is a function that is executed on GPU 'n' number of times  in parallel by 'n' different **CUDA threads**, as opposed to only once like regular functions. A group of threads is called a **CUDA block**. \n",
        "CUDA architecture limits the numbers of threads per block (1024 threads is the limit of per block).\n",
        "\n",
        "Here, we use just in time compiler (jit) with CUDA. `cuda.jit` helps in a  low-level entry point to the CUDA features in Numba. \n",
        "\n",
        "In this function, first we define kernel matrix that has parameters as the input matrices that are copied to work on device memory. We set the number of threads as `cuda.gridsize(1)`; it returns the number of threads that are present in the grid block. We then set the current thread using `cuda.grid(1)`.  Then we carry out the matrix multiplication.\n",
        "\n",
        "The second function takes normal matrices and copies them to device using `cuda.to_device(X)`. Once it is copied then it calls the kernel and passes them to the above function to get the result of matrix multiplication.\n",
        "A note must be taken of blocks per grid means number of blocks and threads per block is same as number of threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0LIt49-B8Y_"
      },
      "source": [
        "@cuda.jit\n",
        "def matmul_kernel(A, B, C):\n",
        "    num_of_threads = cuda.gridsize(1)\n",
        "    curr_id = cuda.grid(1)\n",
        "    rows_num = C.shape[0]\n",
        "    cols_num = C.shape[1]\n",
        "    idx_range = A.shape[1]\n",
        "    for mid in range(curr_id, rows_num*cols_num, num_of_threads):\n",
        "        row = mid // cols_num\n",
        "        col = mid - (row*cols_num)\n",
        "        my_sum = 0.0\n",
        "        for idx in range(0, idx_range):\n",
        "            my_sum += A[row, idx] * B[idx, col]\n",
        "        C[row, col] = my_sum\n",
        "\n",
        "def matmul_gpu(X, Y):\n",
        "    # Allocate the output matrix in GPU memory using cuda.to_device\n",
        "    #\n",
        "    # invoke the dot kernel with 1 threadBlock with 1024 threads\n",
        "    #\n",
        "    # copy the output matrix from GPU to cpu using copy_to_host()\n",
        "    gpu_mat1 = cuda.to_device(X)\n",
        "    gpu_mat2 = cuda.to_device(Y)\n",
        "    res = np.zeros(shape=(X.shape[0], Y.shape[1]), dtype=np.float32)\n",
        "    gpu_mult_res = cuda.to_device(res)\n",
        "    threads_per_block = 1024\n",
        "    blocks_per_grid = 512\n",
        "    matmul_kernel[blocks_per_grid, threads_per_block](\n",
        "        gpu_mat1, gpu_mat2, gpu_mult_res)\n",
        "    mult_res = gpu_mult_res.copy_to_host()\n",
        "    return mult_res"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJe53SJUHBlE"
      },
      "source": [
        "a = np.random.rand(wA,hA)\n",
        "b = np.random.rand(wB,hB)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QapCwvXjM0Y"
      },
      "source": [
        "start = time.perf_counter()\n",
        "gpumatrix1 = matmul_gpu(a,b)\n",
        "end = time.perf_counter()\n",
        "\n",
        "dt_gpu1 = end-start"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7BZ4aG7tBAg"
      },
      "source": [
        "start = time.perf_counter()\n",
        "gpumatrix2 = matmul_gpu(a,b)\n",
        "end = time.perf_counter()\n",
        "\n",
        "dt_gpu2 = end-start"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLzVsXGELno2"
      },
      "source": [
        "start = time.perf_counter()\n",
        "gpumatrix3 = matmul_gpu(a,b)\n",
        "end = time.perf_counter()\n",
        "\n",
        "dt_gpu3 = end-start"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSOSun8HK563"
      },
      "source": [
        "start = time.perf_counter()\n",
        "cpumatrix = np.dot(a,b)\n",
        "end=time.perf_counter()\n",
        "\n",
        "#print results\n",
        "dt_cpu1 = end-start"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eFab94DtMDy"
      },
      "source": [
        "start = time.perf_counter()\n",
        "cpumatrix2 = np.matmul(a,b)\n",
        "end=time.perf_counter()\n",
        "\n",
        "#print results\n",
        "dt_cpu2 = end-start"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbHLFuJDFWaA",
        "outputId": "fdd3c5fc-70e5-4345-c915-dfdb225c856f"
      },
      "source": [
        "print(\"Matrix multipliation first time on GPU = %f s\" % dt_gpu1)\n",
        "print(\"Matrix multipliation second time on GPU = %f s\" % dt_gpu2)\n",
        "print(\"Matrix multipliation third time on GPU = %f s\" % dt_gpu3)\n",
        "print(\"Matrix multipliation on CPU using dot(.) operator in = %f s\" % dt_cpu1)\n",
        "print(\"Matrix multipliation on CPU using matmul function in = %f s\" % dt_cpu2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix multipliation first time on GPU = 44.338945 s\n",
            "Matrix multipliation second time on GPU = 43.917687 s\n",
            "Matrix multipliation third time on GPU = 43.834267 s\n",
            "Matrix multipliation on CPU using dot(.) operator in = 58.588512 s\n",
            "Matrix multipliation on CPU using matmul function in = 58.583081 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY3MX2dQa8JW"
      },
      "source": [
        "We can clearly see the time difference of using GPU and CPU. CPU is 1.5 times slower than GPU in this case. Also, we check the results generated by GPU and CPU are same (rounded upto 4 decimal places). This result indicates that GPU is not only faster but also accurate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6Ih_5y4SMiu",
        "outputId": "8535948a-d753-4594-ab9a-966d2555cf6b"
      },
      "source": [
        "np.allclose(np.around(cpumatrix, decimals=4),np.around(gpumatrix1, decimals=4))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKO8si018jEE",
        "outputId": "c821ce57-779a-4ed3-9866-99ca850e9c12"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep  7 17:04:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    57W / 149W |    441MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEq9bpE5t1bd"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POT131dQaUV3"
   },
   "source": [
    "## Matrix Multiplication using CUDA \n",
    "\n",
    "We know that Matrix operations are very time consuming and Matrix multiplication is one of the prolonged process. To overcome this we try to use matrix multiplication. Here we’ll compare time required to perform Matrix multiplication with CPU and GPU. Also, we’ll try to find time required to compute matrix multiplication of two 1000x1000 matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gn8s0pXHYsw6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numba\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from numba import cuda,jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HX2pCrtUFsdn"
   },
   "outputs": [],
   "source": [
    "wA = 1000\n",
    "hA = 1000\n",
    "wB = hA\n",
    "hB = wA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpRtEPw2T_zt"
   },
   "source": [
    "Firstly, we need to know some basics like what is a kernel, threads and blocks.\n",
    "\n",
    "A **CUDA Kernel** is a function that is executed on GPU 'n' number of times  in parallel by 'n' different **CUDA threads**, as opposed to only once like regular functions. A group of threads is called a **CUDA block**. \n",
    "CUDA architecture limits the numbers of threads per block (1024 threads is the limit of per block).\n",
    "\n",
    "Here, we use just in time compiler (jit) with CUDA. `cuda.jit` helps in a  low-level entry point to the CUDA features in Numba. \n",
    "\n",
    "In this function, first we define kernel matrix that has parameters as the input matrices that are copied to work on device memory. We set the number of threads as `cuda.gridsize(1)`; it returns the number of threads that are present in the grid block. We then set the current thread using `cuda.grid(1)`.  Then we carry out the matrix multiplication.\n",
    "\n",
    "The second function takes normal matrices and copies them to device using `cuda.to_device(X)`. Once it is copied then it calls the kernel and passes them to the above function to get the result of matrix multiplication.\n",
    "A note must be taken of blocks per grid means number of blocks and threads per block is same as number of threads.\n",
    "\n",
    "Once all the operations are over it is important to copy the output result back to CPU and free up the memory on GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "C0LIt49-B8Y_"
   },
   "outputs": [],
   "source": [
    "@cuda.jit(device = True)\n",
    "def matmul_kernel(A, B, C):\n",
    "    num_of_threads = cuda.gridsize(1)\n",
    "    curr_id = cuda.grid(1)\n",
    "    rows_num = C.shape[0]\n",
    "    cols_num = C.shape[1]\n",
    "    idx_range = A.shape[1]\n",
    "    for mid in range(curr_id, rows_num*cols_num, num_of_threads):\n",
    "        row = mid // cols_num\n",
    "        col = mid - (row*cols_num)\n",
    "        my_sum = 0.0\n",
    "        for idx in range(0, idx_range):\n",
    "            my_sum += A[row, idx] * B[idx, col]\n",
    "        C[row, col] = my_sum\n",
    "\n",
    "def matmul_gpu(X, Y):\n",
    "    # Allocate the output matrix in GPU memory using cuda.to_device\n",
    "    #\n",
    "    # invoke the dot kernel with 1 threadBlock with 1024 threads\n",
    "    #\n",
    "    # copy the output matrix from GPU to cpu using copy_to_host()\n",
    "    gpu_mat1 = cuda.to_device(X)\n",
    "    gpu_mat2 = cuda.to_device(Y)\n",
    "    res = np.zeros(shape=(X.shape[0], Y.shape[1]), dtype=np.float32)\n",
    "    gpu_mult_res = cuda.to_device(res)\n",
    "    threads_per_block = 1024\n",
    "    blocks_per_grid = 1\n",
    "    matmul_kernel[blocks_per_grid, threads_per_block](\n",
    "        gpu_mat1, gpu_mat2, gpu_mult_res)\n",
    "    mult_res = gpu_mult_res.copy_to_host()\n",
    "    return mult_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hb1hBPXZIRbe"
   },
   "outputs": [],
   "source": [
    "def cpumul(matrix1,matrix2,rmatrix):\n",
    "  for i in range(len(matrix1)):\n",
    "    for j in range(len(matrix2[0])):\n",
    "      for k in range(len(matrix2)):\n",
    "        rmatrix[i][j] += matrix1[i][k] * matrix2[k][j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sJe53SJUHBlE"
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(wA,hA)\n",
    "b = np.random.rand(wB,hB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-QapCwvXjM0Y"
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "gpumatrix = matmul_gpu(a,b)\n",
    "end = time.perf_counter()\n",
    "\n",
    "dt_gpu = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NSOSun8HK563"
   },
   "outputs": [],
   "source": [
    "cpumatrix = np.zeros(shape=(wA,hB))\n",
    "\n",
    "start = time.perf_counter()\n",
    "cpumul(a,b,cpumatrix)\n",
    "end=time.perf_counter()\n",
    "\n",
    "#print results\n",
    "dt_cpu = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbHLFuJDFWaA",
    "outputId": "306bf1d8-9b38-4336-c5b8-7c00b22c182a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multipliation on GPU = 1.073255 s\n",
      "Matrix multipliation on CPU in = 1204.165365 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix multipliation on GPU = %f s\" % dt_gpu)\n",
    "print(\"Matrix multipliation on CPU in = %f s\" % dt_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY3MX2dQa8JW"
   },
   "source": [
    "We can clearly see the time difference of using GPU and CPU. CPU is 1000 times slower than GPU in this case. Also, we check the results generated by GPU and CPU are same (rounded upto 4 decimal places). This result indicates that GPU is not only faster but also accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6Ih_5y4SMiu",
    "outputId": "ba12d606-a323-4225-c6f2-668e46f9a624"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(np.around(cpumatrix, decimals=4),np.around(gpumatrix, decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKO8si018jEE",
    "outputId": "a713bb72-1805-47db-983a-049e59749a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep  3 20:39:16 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   51C    P0    28W /  70W |    124MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ankit_matrix_mul.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
